{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Llama 3\n",
    "llm = Groq(model=\"llama-3.1-70b-versatile\")\n",
    "Settings.llm = llm\n",
    "\n",
    "def get_doc_tools(file_path: str) -> str: \n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    # summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # summary_query_engine = summary_index.as_query_engine(\n",
    "    #     response_mode=\"tree_summarize\",\n",
    "    #     use_async=True,\n",
    "    # )\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "    # summary_tool = QueryEngineTool.from_defaults(\n",
    "    #     query_engine=summary_query_engine,\n",
    "    #     description=(\n",
    "    #         \"Useful for summarization questions related to paul_graham_essay\"\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Useful for retrieving specific context from the paul_graham_essay.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # return(vector_tool, summary_tool)\n",
    "    return(vector_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool = get_doc_tools(\"data/codet_avaliacao.pdf\")\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Responda as seguintes perguntas sobre o CodeT:\n",
      "    - What is the problem CodeT is solving?\n",
      "    - Why is solving this problem important?\n",
      "    - What is CodeT's contribution?\n",
      "    - Explain CodeT method\n",
      "    - How does CodeT perform compared to other frameworks?\n",
      "    \n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is the problem CodeT is solving?\"}\n",
      "=== Function Output ===\n",
      "CodeT is solving the problem of code generation and test case generation for programming problems, specifically by leveraging pre-trained language models to generate both code solutions and test cases, and then selecting the best solution based on the dual execution agreement.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Why is solving this problem important?\"}\n",
      "=== Function Output ===\n",
      "Solving this problem is important because it can help improve the performance of code generation models, such as Codex, I NCODER, and C ODEGEN, by identifying the best code solutions that match the desired functionality. This can be particularly useful in real-world applications where the quality of the generated code is crucial.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is CodeT's contribution?\"}\n",
      "=== Function Output ===\n",
      "CodeT's contribution is that it can significantly improve the performance of code generation models, especially in the one-shot setting, by filtering out low-quality solutions and scoring consensus sets.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain CodeT method\"}\n",
      "=== Function Output ===\n",
      "The CodeT method is a novel approach that leverages pre-trained language models to generate both code solutions and test cases for a given programming problem. The method works as follows:\n",
      "\n",
      "1. The pre-trained language model generates multiple diverse code samples for the given problem.\n",
      "2. The same pre-trained language model is used to automatically generate test cases for the code samples.\n",
      "3. The code samples are executed using the generated test cases.\n",
      "4. A dual execution agreement is performed, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.\n",
      "5. The best solution is selected based on the dual execution agreement.\n",
      "\n",
      "The CodeT method aims to reduce the human effort and increase the coverage of test scenarios by automatically generating test cases for the code samples. This approach has shown significant improvements in code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"How does CodeT perform compared to other frameworks?\"}\n",
      "=== Function Output ===\n",
      "CodeT consistently outperforms other frameworks, such as AlphaCode-C, and achieves significant improvements over baseline models. It demonstrates superiority in various benchmarks, including HumanEval, MBPP, APPS, and CodeContests, with absolute improvements in the range of 4.2% to 13.1% and even up to 20% in some cases.\n",
      "=== LLM Response ===\n",
      "The CodeT method is a novel approach that leverages pre-trained language models to generate both code solutions and test cases for a given programming problem. The method works as follows:\n",
      "\n",
      "1. The pre-trained language model generates multiple diverse code samples for the given problem.\n",
      "2. The same pre-trained language model is used to automatically generate test cases for the code samples.\n",
      "3. The code samples are executed using the generated test cases.\n",
      "4. A dual execution agreement is performed, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.\n",
      "5. The best solution is selected based on the dual execution agreement.\n",
      "\n",
      "The CodeT method aims to reduce the human effort and increase the coverage of test scenarios by automatically generating test cases for the code samples. This approach has shown significant improvements in code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks.\n",
      "\n",
      "CodeT consistently outperforms other frameworks, such as AlphaCode-C, and achieves significant improvements over baseline models. It demonstrates superiority in various benchmarks, including HumanEval, MBPP, APPS, and CodeContests, with absolute improvements in the range of 4.2% to 13.1% and even up to 20% in some cases.\n",
      "\n",
      "Solving this problem is important because it can help improve the performance of code generation models, such as Codex, I NCODER, and C ODEGEN, by identifying the best code solutions that match the desired functionality. This can be particularly useful in real-world applications where the quality of the generated code is crucial.\n",
      "\n",
      "CodeT's contribution is that it can significantly improve the performance of code generation models, especially in the one-shot setting, by filtering out low-quality solutions and scoring consensus sets.\n",
      "\n",
      "CodeT is solving the problem of code generation and test case generation for programming problems, specifically by leveraging pre-trained language models to generate both code solutions and test cases, and then selecting the best solution based on the dual execution agreement.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"The CodeT method is a novel approach that leverages pre-trained language models to generate both code solutions and test cases for a given programming problem. The method works as follows:\\n\\n1. The pre-trained language model generates multiple diverse code samples for the given problem.\\n2. The same pre-trained language model is used to automatically generate test cases for the code samples.\\n3. The code samples are executed using the generated test cases.\\n4. A dual execution agreement is performed, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.\\n5. The best solution is selected based on the dual execution agreement.\\n\\nThe CodeT method aims to reduce the human effort and increase the coverage of test scenarios by automatically generating test cases for the code samples. This approach has shown significant improvements in code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks.\\n\\nCodeT consistently outperforms other frameworks, such as AlphaCode-C, and achieves significant improvements over baseline models. It demonstrates superiority in various benchmarks, including HumanEval, MBPP, APPS, and CodeContests, with absolute improvements in the range of 4.2% to 13.1% and even up to 20% in some cases.\\n\\nSolving this problem is important because it can help improve the performance of code generation models, such as Codex, I NCODER, and C ODEGEN, by identifying the best code solutions that match the desired functionality. This can be particularly useful in real-world applications where the quality of the generated code is crucial.\\n\\nCodeT's contribution is that it can significantly improve the performance of code generation models, especially in the one-shot setting, by filtering out low-quality solutions and scoring consensus sets.\\n\\nCodeT is solving the problem of code generation and test case generation for programming problems, specifically by leveraging pre-trained language models to generate both code solutions and test cases, and then selecting the best solution based on the dual execution agreement.\", sources=[ToolOutput(content='CodeT is solving the problem of code generation and test case generation for programming problems, specifically by leveraging pre-trained language models to generate both code solutions and test cases, and then selecting the best solution based on the dual execution agreement.', tool_name='query_engine_tool', raw_input={'input': 'What is the problem CodeT is solving?'}, raw_output=Response(response='CodeT is solving the problem of code generation and test case generation for programming problems, specifically by leveraging pre-trained language models to generate both code solutions and test cases, and then selecting the best solution based on the dual execution agreement.', source_nodes=[NodeWithScore(node=TextNode(id_='4e1c4865-e1e4-401a-a65d-eae76ba400c7', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f61b0c20-6547-41b7-a4cf-4afc6e1a2ac7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e5272d1a-9acf-407d-9ff3-1296fb4123fc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description ‚Äúsum(first index value, last index value)‚Äù is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6393933025106274), NodeWithScore(node=TextNode(id_='c6ab7a7c-98d6-4cc4-9122-71139ee239d6', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3787fa80-36ce-4a22-950c-4df285e9b89d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5088384e-f9a6-4d47-b28d-5233602f2f64', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0ùë•1\\nùë•2\\nùë•3ùë¶1\\nùë¶2\\nùë¶3\\nùë¶4\\nùë¶5 returna ùë•4\\nFigure 3: A simple example of the programming\\nproblem ‚Äúreturn the square of a number‚Äù. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x‚àà S x, y‚àà S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n‚Ä¢ We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ÀÜxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x‚ààX, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,¬∑¬∑¬∑,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x‚àà Sx, y‚àà Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6348175268840989)], metadata={'4e1c4865-e1e4-401a-a65d-eae76ba400c7': {'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'c6ab7a7c-98d6-4cc4-9122-71139ee239d6': {'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content='Solving this problem is important because it can help improve the performance of code generation models, such as Codex, I NCODER, and C ODEGEN, by identifying the best code solutions that match the desired functionality. This can be particularly useful in real-world applications where the quality of the generated code is crucial.', tool_name='query_engine_tool', raw_input={'input': 'Why is solving this problem important?'}, raw_output=Response(response='Solving this problem is important because it can help improve the performance of code generation models, such as Codex, I NCODER, and C ODEGEN, by identifying the best code solutions that match the desired functionality. This can be particularly useful in real-world applications where the quality of the generated code is crucial.', source_nodes=[NodeWithScore(node=TextNode(id_='c6ab7a7c-98d6-4cc4-9122-71139ee239d6', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3787fa80-36ce-4a22-950c-4df285e9b89d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5088384e-f9a6-4d47-b28d-5233602f2f64', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0ùë•1\\nùë•2\\nùë•3ùë¶1\\nùë¶2\\nùë¶3\\nùë¶4\\nùë¶5 returna ùë•4\\nFigure 3: A simple example of the programming\\nproblem ‚Äúreturn the square of a number‚Äù. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x‚àà S x, y‚àà S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n‚Ä¢ We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ÀÜxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x‚ààX, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,¬∑¬∑¬∑,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x‚àà Sx, y‚àà Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5202288330427287), NodeWithScore(node=TextNode(id_='baf53c6d-888b-442b-a9b1-091201e34b2d', embedding=None, metadata={'page_label': '14', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1833dd33-7421-4359-a1e2-b1bd696091af', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '14', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f1feae1334cac074d03118a38e9eb391e1397303e6cffdcd8852c372d1dd8dd2')}, text='Published as a conference paper at ICLR 2023\\nDe-duplication HumanEval MBPP\\nSolution Test 1 2 10 1 2 10\\nNo No 44.5 50 .1 65 .755.4 61.7 72 .7\\nNo Yes 42.2 48 .866.7 54.562.3 73.4\\nYes No 46.9 52.5 65.6 54 .7 61 .7 73 .2\\nYes Yes 42.7 51 .2 66 .4 54 .7 62 .1 73 .2\\nTable 7: Pass@ k(%) on the HumanEval and MBPP benchmarks using C ODET and code-cushman-\\n001 with different de-duplication settings. The setting ‚ÄúNo No‚Äù in the first line means that neither\\nthe code solutions nor the test cases are de-duplicated, which is used in our main experiments.\\nMethods C ODET C ODET (Remove Trivial)\\nk 1 10 100 1 10 100\\nAPPSINTRODUCTORY 34.653.2- 34.90.353.40.2-\\nINTERVIEW 8.1 18.1- 8.30.2 18.20.1-\\nCOMPETITION 2.2 8.6 - 2.50.3 8.70.1 -\\nCodeContests 2.1 5.3 9.92.70.6 5.30.0 10.00.1\\nTable 8: Pass@ k(%) results on the zero-shot APPS and CodeContests benchmarks using code-\\ndavinci-002 and C ODET with/without the trivial code solutions filtered. The numbers in red indicate\\nthe absolute improvements after filtering the trivial solutions.\\nfind that de-duplication has slight and inconsistent influence on the performance of C ODET. For the\\nHumanEval benchmark, the pass@ 1results using code solution de-duplication alone are better than\\nother settings. Nonetheless, for the MBPP benchmark, the best pass@ 1results are achieved without\\nde-duplication. Therefore, in our main experiments, we reserve all the generated code solutions and\\ntest cases when performing C ODET and leave the study of more advanced de-duplication methods\\nfor future work.\\nE S ENSITIVITY TO THE TEMPERATURE\\nThe hyper-parameter temperature has a great impact on the quality of generated code solutions and\\ntest cases when using top psampling. We use a high temperature of 0.8in our main experiments\\nsince C ODET could benefit from a larger number of diverse samples. To investigate the sensitivity\\nof C ODET to the temperature, we perform an ablation study by using a range of temperatures to\\nreport the results of baseline pass@ 100and C ODET pass@ 1. Figure 9 shows the results of code-\\ncushman-001 on the HumanEval benchmark at different temperature settings. We can find that a\\nhigher temperature does improve the baseline pass@ 100and C ODET pass@ 1, and C ODET achieves\\na good performance when temperature is set to 0.8.\\nF R EMOVING TRIVIAL CODE SOLUTIONS\\nThe problems in the APPS C OMPETITION and CodeContests benchmarks are of great difficulty\\ncompared to HumanEval and MBPP, leading to the poor performance of the most capable code-\\ndavinci-002 model. After checking the incorrect code solutions generated by code-davinci-002, we\\nidentify many trivial solutions that just return the input argument or a constant value. Such solutions\\nmay hinder the ranking process of C ODET if they can pass any generated test case. A trivial solution\\ncan be easily identified by its input arguments and returned values. If a solution always returns the\\nsame output value for different inputs, or its returned values are always the same as the inputs, it\\nmust be a trivial solution. To investigate the impact of trivial code solutions, we use code-davinci-\\n002 on the zero-shot APPS and CodeContests benchmarks, and perform C ODET after filtering out\\nall the trivial solutions. As a result, we can remove an average of 4.5(91.6) trivial solutions from\\nthe50(1,000) generated solutions per problem for the APPS (CodeContests) benchmark. How-\\n14', mimetype='text/plain', start_char_idx=0, end_char_idx=3392, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.49985257941066724)], metadata={'c6ab7a7c-98d6-4cc4-9122-71139ee239d6': {'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'baf53c6d-888b-442b-a9b1-091201e34b2d': {'page_label': '14', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content=\"CodeT's contribution is that it can significantly improve the performance of code generation models, especially in the one-shot setting, by filtering out low-quality solutions and scoring consensus sets.\", tool_name='query_engine_tool', raw_input={'input': \"What is CodeT's contribution?\"}, raw_output=Response(response=\"CodeT's contribution is that it can significantly improve the performance of code generation models, especially in the one-shot setting, by filtering out low-quality solutions and scoring consensus sets.\", source_nodes=[NodeWithScore(node=TextNode(id_='08258cc2-fd2b-4b24-acac-670ef7236fb4', embedding=None, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fbafecf0-3bbb-480b-a2f9-969fca82bf66', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='91da99ff59e8f9d8547e5c131dc76afe5ef0c57ba8e0c220cb8a1a6cf6aab276')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 100 1 2 10\\ncode-cushman-001 31.7‚àí1.856.42.184.16.758.614.165.715.680.114.4\\ncode-davinci-001 34.8‚àí4.263.02.487.23.160.410.269.110.282.46.6\\ncode-davinci-002 47.60.6 78.83.992.70.674.89.082.97.889.02.4\\nTable 6: Pass@ k(%) on the original HumanEval benchmark with Codex models. The numbers in\\norange indicate the absolute improvements of pass@ kon the original benchmark over our modified\\nbenchmark in Table 2.\\nA M ORE IMPLEMENTATION DETAILS\\nWe set the temperature to 0.8, the top pto0.95, the max generation length to 300, and the timeout\\nof executing a test case to 0.1seconds. Specially, for baseline pass@ 1, we use the greedy search\\nsetting with temperature 0. The number of sampling test cases for each problem is set to 100for the\\nHumanEval and MBPP benchmarks, and 50for the APPS and CodeContests benchmarks. When\\nscoring consensus sets in C ODET, we use the square root of |Sx|to reduce the impact caused by code\\nsolutions. A supporting experiment can be found in Appendix C. For code solution post-processing,\\nwe follow Chen et al. (2021) to truncate the generated content by five stop sequences: ‚Äú \\\\nclass ‚Äù,\\n‚Äú\\\\ndef ‚Äù, ‚Äú\\\\n#‚Äù, ‚Äú\\\\nif‚Äù, and ‚Äú \\\\nprint ‚Äù. For the implementation of I NCODER and C ODEGEN, we\\nuse the HuggingFace transformers library (Wolf et al., 2019) and run both models with half preci-\\nsion. In addition, when the number of consensus sets in C ODET is smaller than k, the selection is\\ndone from the highest scoring consensus set to the lowest. When reaching the set with the lowest\\nscore, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets\\nis larger than k, as shown in Figure 6.\\nB R ESULTS ON ORIGINAL HUMAN EVAL\\nAs mentioned in Section 3, for all benchmarks, we remove the example input-output cases from the\\noriginal contexts to avoid exposing real test cases. To study the influence of such modification, we\\ntake HumanEval as an example and perform an additional experiment with its original contexts. The\\nresults are summarized in Table 6. On the one hand, the baseline pass@ 10and pass@ 100results on\\nthe original HumanEval benchmark outperform the modified version, which is reasonable because\\nthe example input-output cases may provide useful information for code generation. Nevertheless,\\nthe pass@ 1results on the original benchmark are basically the same or even worse than the modified\\nversion, suggesting that the Codex models have not fully understood the semantics of the example\\ninput-output cases provided in the contexts. On the other hand, the performance of C ODET is sig-\\nnificantly improved using the original benchmark. This is as expected because the original contexts\\nused for test case generation include real test cases, which could be borrowed by the models during\\nthe generation. Such real test cases will greatly empower C ODET to distinguish correct code solu-\\ntions. Hence, in our experiments, it is indispensable to remove the example input-output cases to\\navoid exposing the real test cases. In this way, the effectiveness of C ODET can be fairly verified.\\nC A NALYSIS ON CODE SOLUTIONS\\nIn C ODET, code solutions that can pass exactly the same test cases are considered consistent in\\nfunctionality and are grouped into the same consensus set. Since we employ top psampling with a\\nrather high temperature of 0.8, the functionality of the code solutions may vary significantly, which\\nresults in more consensus sets. We draw a histogram in Figure 6 to show the number of consensus\\nsets produced by code-cushman-001 and C ODET for each problem on the HumanEval benchmark.\\nThe average and median numbers are 26.8and25.5, respectively. We can find that most problems\\nhave less than 50consensus sets, but the numbers have a high variance among different problems.\\nWe also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in\\nFigure 7. The consensus sets ranked top 1tend to have more code solutions with an average value\\nof9.8, and the numbers also have a high variance.\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=4081, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5668586343920633), NodeWithScore(node=TextNode(id_='0b752686-4982-49af-8767-10196de84c77', embedding=None, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='90b99c45-1691-4cd8-ae53-0c8f732593d7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='e3f991319b58b3204c80f0bce1f2abc8e40b518985fabe35de7031f9cc5260fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3f65451a-f459-4d18-8308-fbb15bc8e031', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3cde2d44b1a7c1fa8c444185cab37c8caa66b7c3892b462f55831a7a84271495')}, text='Published as a conference paper at ICLR 2023\\nk 1 10 50 100 1000 1 2 10 100\\nBaseline C ODET\\nAPPSINTRODUCTORY 29.348.5 60 .9- - 47.318.052.758.49.9 -\\nINTERVIEW 6.4 14.6 25 .4- - 14.37.918.223.38.7 -\\nCOMPETITION 2.5 6.3 14 .5- - 6.23.7 9.8 13.67.3 -\\nCodeContests 1.0 4.1 7 .1 8.8 15 .23.22.2 5.6 9.35.2 12.33.5\\nBaseline Filter C ODET Filter\\nAPPSINTRODUCTORY 43.658.6- - - 49.66.054.359.40.8 -\\nINTERVIEW 13.322.8- - - 16.12.819.524.01.2 -\\nCOMPETITION 7.0 13.3- - - 7.90.9 10.514.10.8 -\\nCodeContests 9.9 14.5 15 .115.2- 9.6‚àí0.311.513.7‚àí0.814.5‚àí0.7\\nTable 9: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nand the one-shot setting. The numbers in red indicate the absolute improvements of C ODET (Filter)\\nover Baseline (Filter) on pass@ 1, pass@ 10and pass@ 100. For C ODET (Filter), temperature is set\\nto0.8and sampling number is set to 50for APPS and 1,000for CodeContests. We do not report\\npass@ 1000 for ‚ÄúBaseline Filter‚Äù because the numbers of code solutions after filtering are less than\\nthe sampling numbers.\\never, as shown in Table 8, after removing a prominent percentage of trivial solutions, there is little\\nperformance gain, which could exactly demonstrate the robustness of C ODET.\\nG R ESULTS ON APPS AND CODECONTESTS IN THE ONE-SHOT SETTING\\nInspired by Chen et al. (2021) and Li et al. (2022b), we build one-shot versions of APPS and Code-\\nContests by appending a single input-output example to the problem description as a formatting\\nhint. After generation, we filter out the generated solutions that cannot pass the given example\\ninput-output cases, which we call the ‚ÄúBaseline Filter‚Äù method. After filtering, we can still perform\\nCODET using the rest of code solutions, called the ‚ÄúC ODET Filter‚Äù method. Following the zero-\\nshot experiments on APPS and CodeContests, we employ code-davinci-002 for generation and set\\nthe sampling number to 50for APPS and 1,000for CodeContests.\\nWe summarize the experimental results in Table 9, where we can find the one-shot performance\\nusing C ODET is much better than that reported in Table 3 in the zero-shot setting. The performance\\nof the baselines can be significantly improved by filtering the solutions with the given example test\\ncases. Moreover, ‚ÄúC ODET Filter‚Äù can further outperform ‚ÄúBaseline Filter‚Äù on the APPS benchmark,\\nespecially for the introductory and interview problems. Nonetheless, for CodeContests and the\\ncompetition level problems in APPS, ‚ÄúC ODET Filter‚Äù has little performance improvement or even\\nperforms slightly worse than ‚ÄúBaseline Filter‚Äù. After manual investigation, we blame such issue to\\nthe generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the\\ninterest of future study on test case generation for more challenging programming problems.\\nH M ORE ANALYSIS ON TESTCASES\\nH.1 S TATISTICS ON TESTCASES\\nHow many valid test cases do the models generate for C ODET? Taking the HumanEval benchmark\\nas an example, we sample 100times for each problem when generating test cases. As illustrated\\nin Figure 2, at each time of sampling, we feed the context calong with an instruction pto the\\nmodel and get the generated content that may contain multiple test cases. Then, as mentioned\\nin Section 4.3, we further post-process the generated samples to get individual test cases that are\\nsyntactically correct.', mimetype='text/plain', start_char_idx=0, end_char_idx=3351, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5604883565453653)], metadata={'08258cc2-fd2b-4b24-acac-670ef7236fb4': {'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, '0b752686-4982-49af-8767-10196de84c77': {'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content='The CodeT method is a novel approach that leverages pre-trained language models to generate both code solutions and test cases for a given programming problem. The method works as follows:\\n\\n1. The pre-trained language model generates multiple diverse code samples for the given problem.\\n2. The same pre-trained language model is used to automatically generate test cases for the code samples.\\n3. The code samples are executed using the generated test cases.\\n4. A dual execution agreement is performed, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.\\n5. The best solution is selected based on the dual execution agreement.\\n\\nThe CodeT method aims to reduce the human effort and increase the coverage of test scenarios by automatically generating test cases for the code samples. This approach has shown significant improvements in code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks.', tool_name='query_engine_tool', raw_input={'input': 'Explain CodeT method'}, raw_output=Response(response='The CodeT method is a novel approach that leverages pre-trained language models to generate both code solutions and test cases for a given programming problem. The method works as follows:\\n\\n1. The pre-trained language model generates multiple diverse code samples for the given problem.\\n2. The same pre-trained language model is used to automatically generate test cases for the code samples.\\n3. The code samples are executed using the generated test cases.\\n4. A dual execution agreement is performed, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples.\\n5. The best solution is selected based on the dual execution agreement.\\n\\nThe CodeT method aims to reduce the human effort and increase the coverage of test scenarios by automatically generating test cases for the code samples. This approach has shown significant improvements in code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks.', source_nodes=[NodeWithScore(node=TextNode(id_='4e1c4865-e1e4-401a-a65d-eae76ba400c7', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f61b0c20-6547-41b7-a4cf-4afc6e1a2ac7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e5272d1a-9acf-407d-9ff3-1296fb4123fc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description ‚Äúsum(first index value, last index value)‚Äù is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6541882192111425), NodeWithScore(node=TextNode(id_='acd6646f-4cec-4670-baf6-46681bc72bdf', embedding=None, metadata={'page_label': '1', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0b61254-cdd3-4852-820d-ead70e4148f5', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='8d1d789774d04ee7a7d733e88e2c058730b81bb7f62789f6e367965b7e6eb45e')}, text='Published as a conference paper at ICLR 2023\\nCODET: C ODE GENERATION WITH GENERATED TESTS\\nBei Chen‚àó, Fengji Zhang‚àó, Anh Nguyen‚àó, Daoguang Zan, Zeqi Lin,\\nJian-Guang Lou, Weizhu Chen\\nMicrosoft Corporation\\n{beichen, v-fengjzhang, anhnguyen, v-dazan,\\nzeqi.lin, jlou, wzchen }@microsoft.com\\nABSTRACT\\nThe task of generating code solutions for a given programming problem can bene-\\nfit from the use of pre-trained language models such as Codex, which can produce\\nmultiple diverse samples. However, a major challenge for this task is to select\\nthe most appropriate solution from the multiple samples generated by the pre-\\ntrained language models. A natural way to evaluate the quality and correctness\\nof a code solution is to run it against a set of test cases, but the manual creation\\nof such test cases is often costly and time-consuming. In this paper, we propose\\na novel method, C ODET, that leverages the same pre-trained language models to\\nautomatically generate test cases for the code samples, thus reducing the human\\neffort and increasing the coverage of the test scenarios. C ODET then executes the\\ncode samples using the generated test cases and performs a dual execution agree-\\nment, which considers both the consistency of the outputs against the generated\\ntest cases and the agreement of the outputs with other code samples. We con-\\nduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS,\\nand CodeContests, using five different pre-trained language models with varying\\nsizes and capabilities. Our results show that C ODET can significantly improve the\\nperformance of code solution selection over previous methods, achieving remark-\\nable and consistent gains across different models and benchmarks. For instance,\\nCODET improves the pass@ 1metric on HumanEval to 65.8%, which represents\\nan absolute improvement of 18.8%over the code-davinci-002 model, and an ab-\\nsolute improvement of more than 20% over the previous state-of-the-art results.\\n1 I NTRODUCTION\\nDespite the remarkable progress in pre-training techniques for code generation, selecting a single\\ncorrect solution from multiple candidates generated by large language models remains a hard prob-\\nlem. For instance, Codex (Chen et al., 2021), a state-of-the-art pre-trained language model for code\\ngeneration, can achieve a pass @100 (pass if one or more among 100generated solutions for a given\\nproblem can pass the corresponding test cases) of 77.4%, but a pass @1(correct rate of a single so-\\nlution) of only 33.5%on the HumanEval benchmark (Chen et al., 2021)1. This huge gap limits the\\npractical usefulness of code generation models and motivates us to explore how to pick the correct\\nor best solution from multiple candidates.\\nA straightforward way to verify the correctness of a solution is to execute it and check if it passes\\nall corresponding test cases. This execution-guided approach has been widely adopted in various\\ncode-related tasks, such as code generation (Chen et al., 2021; Li et al., 2022b; Shi et al., 2022),\\ncode translation (Roziere et al., 2021), and program synthesis (Chen et al., 2018; Ellis et al., 2019).\\nHowever, this approach relies heavily on the quality and quantity of test cases, which are often costly\\nand time-consuming to create and maintain. Moreover, in real-world applications like Copilot2, a\\ncode generation tool that assists developers in writing code, it is unrealistic to expect users to provide\\ntest cases for every problem they want to solve. Therefore, we propose to automatically generate\\ntest cases for arbitrary programming problems and use them to quickly verify any solution.\\n‚àóThe first three authors contributed equally.\\n1We report the results on the HumanEval benchmark with the Codex model code-cushman-001. More\\nresults with different models and benchmarks can be found in Section 4.1 and 4.2\\n2https://github.com/features/copilot\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=3872, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6521034157719083)], metadata={'4e1c4865-e1e4-401a-a65d-eae76ba400c7': {'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'acd6646f-4cec-4670-baf6-46681bc72bdf': {'page_label': '1', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content='CodeT consistently outperforms other frameworks, such as AlphaCode-C, and achieves significant improvements over baseline models. It demonstrates superiority in various benchmarks, including HumanEval, MBPP, APPS, and CodeContests, with absolute improvements in the range of 4.2% to 13.1% and even up to 20% in some cases.', tool_name='query_engine_tool', raw_input={'input': 'How does CodeT perform compared to other frameworks?'}, raw_output=Response(response='CodeT consistently outperforms other frameworks, such as AlphaCode-C, and achieves significant improvements over baseline models. It demonstrates superiority in various benchmarks, including HumanEval, MBPP, APPS, and CodeContests, with absolute improvements in the range of 4.2% to 13.1% and even up to 20% in some cases.', source_nodes=[NodeWithScore(node=TextNode(id_='195f22c9-84bc-4fb0-bd86-9180c2d5e90a', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='71b21d63-660e-4bc1-a6d7-76cb06fa58bc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f2ebe392-8f46-4969-b5e3-1ad135b59841', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6909084127555096), NodeWithScore(node=TextNode(id_='f2ebe392-8f46-4969-b5e3-1ad135b59841', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='71b21d63-660e-4bc1-a6d7-76cb06fa58bc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='195f22c9-84bc-4fb0-bd86-9180c2d5e90a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6865730581017966)], metadata={'195f22c9-84bc-4fb0-bd86-9180c2d5e90a': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'f2ebe392-8f46-4969-b5e3-1ad135b59841': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='4e1c4865-e1e4-401a-a65d-eae76ba400c7', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f61b0c20-6547-41b7-a4cf-4afc6e1a2ac7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e5272d1a-9acf-407d-9ff3-1296fb4123fc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description ‚Äúsum(first index value, last index value)‚Äù is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6393933025106274), NodeWithScore(node=TextNode(id_='c6ab7a7c-98d6-4cc4-9122-71139ee239d6', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3787fa80-36ce-4a22-950c-4df285e9b89d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5088384e-f9a6-4d47-b28d-5233602f2f64', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0ùë•1\\nùë•2\\nùë•3ùë¶1\\nùë¶2\\nùë¶3\\nùë¶4\\nùë¶5 returna ùë•4\\nFigure 3: A simple example of the programming\\nproblem ‚Äúreturn the square of a number‚Äù. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x‚àà S x, y‚àà S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n‚Ä¢ We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ÀÜxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x‚ààX, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,¬∑¬∑¬∑,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x‚àà Sx, y‚àà Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6348175268840989), NodeWithScore(node=TextNode(id_='c6ab7a7c-98d6-4cc4-9122-71139ee239d6', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3787fa80-36ce-4a22-950c-4df285e9b89d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5088384e-f9a6-4d47-b28d-5233602f2f64', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0ùë•1\\nùë•2\\nùë•3ùë¶1\\nùë¶2\\nùë¶3\\nùë¶4\\nùë¶5 returna ùë•4\\nFigure 3: A simple example of the programming\\nproblem ‚Äúreturn the square of a number‚Äù. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x‚àà S x, y‚àà S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n‚Ä¢ We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ÀÜxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x‚ààX, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,¬∑¬∑¬∑,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x‚àà Sx, y‚àà Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5202288330427287), NodeWithScore(node=TextNode(id_='baf53c6d-888b-442b-a9b1-091201e34b2d', embedding=None, metadata={'page_label': '14', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1833dd33-7421-4359-a1e2-b1bd696091af', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '14', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f1feae1334cac074d03118a38e9eb391e1397303e6cffdcd8852c372d1dd8dd2')}, text='Published as a conference paper at ICLR 2023\\nDe-duplication HumanEval MBPP\\nSolution Test 1 2 10 1 2 10\\nNo No 44.5 50 .1 65 .755.4 61.7 72 .7\\nNo Yes 42.2 48 .866.7 54.562.3 73.4\\nYes No 46.9 52.5 65.6 54 .7 61 .7 73 .2\\nYes Yes 42.7 51 .2 66 .4 54 .7 62 .1 73 .2\\nTable 7: Pass@ k(%) on the HumanEval and MBPP benchmarks using C ODET and code-cushman-\\n001 with different de-duplication settings. The setting ‚ÄúNo No‚Äù in the first line means that neither\\nthe code solutions nor the test cases are de-duplicated, which is used in our main experiments.\\nMethods C ODET C ODET (Remove Trivial)\\nk 1 10 100 1 10 100\\nAPPSINTRODUCTORY 34.653.2- 34.90.353.40.2-\\nINTERVIEW 8.1 18.1- 8.30.2 18.20.1-\\nCOMPETITION 2.2 8.6 - 2.50.3 8.70.1 -\\nCodeContests 2.1 5.3 9.92.70.6 5.30.0 10.00.1\\nTable 8: Pass@ k(%) results on the zero-shot APPS and CodeContests benchmarks using code-\\ndavinci-002 and C ODET with/without the trivial code solutions filtered. The numbers in red indicate\\nthe absolute improvements after filtering the trivial solutions.\\nfind that de-duplication has slight and inconsistent influence on the performance of C ODET. For the\\nHumanEval benchmark, the pass@ 1results using code solution de-duplication alone are better than\\nother settings. Nonetheless, for the MBPP benchmark, the best pass@ 1results are achieved without\\nde-duplication. Therefore, in our main experiments, we reserve all the generated code solutions and\\ntest cases when performing C ODET and leave the study of more advanced de-duplication methods\\nfor future work.\\nE S ENSITIVITY TO THE TEMPERATURE\\nThe hyper-parameter temperature has a great impact on the quality of generated code solutions and\\ntest cases when using top psampling. We use a high temperature of 0.8in our main experiments\\nsince C ODET could benefit from a larger number of diverse samples. To investigate the sensitivity\\nof C ODET to the temperature, we perform an ablation study by using a range of temperatures to\\nreport the results of baseline pass@ 100and C ODET pass@ 1. Figure 9 shows the results of code-\\ncushman-001 on the HumanEval benchmark at different temperature settings. We can find that a\\nhigher temperature does improve the baseline pass@ 100and C ODET pass@ 1, and C ODET achieves\\na good performance when temperature is set to 0.8.\\nF R EMOVING TRIVIAL CODE SOLUTIONS\\nThe problems in the APPS C OMPETITION and CodeContests benchmarks are of great difficulty\\ncompared to HumanEval and MBPP, leading to the poor performance of the most capable code-\\ndavinci-002 model. After checking the incorrect code solutions generated by code-davinci-002, we\\nidentify many trivial solutions that just return the input argument or a constant value. Such solutions\\nmay hinder the ranking process of C ODET if they can pass any generated test case. A trivial solution\\ncan be easily identified by its input arguments and returned values. If a solution always returns the\\nsame output value for different inputs, or its returned values are always the same as the inputs, it\\nmust be a trivial solution. To investigate the impact of trivial code solutions, we use code-davinci-\\n002 on the zero-shot APPS and CodeContests benchmarks, and perform C ODET after filtering out\\nall the trivial solutions. As a result, we can remove an average of 4.5(91.6) trivial solutions from\\nthe50(1,000) generated solutions per problem for the APPS (CodeContests) benchmark. How-\\n14', mimetype='text/plain', start_char_idx=0, end_char_idx=3392, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.49985257941066724), NodeWithScore(node=TextNode(id_='08258cc2-fd2b-4b24-acac-670ef7236fb4', embedding=None, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fbafecf0-3bbb-480b-a2f9-969fca82bf66', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='91da99ff59e8f9d8547e5c131dc76afe5ef0c57ba8e0c220cb8a1a6cf6aab276')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 100 1 2 10\\ncode-cushman-001 31.7‚àí1.856.42.184.16.758.614.165.715.680.114.4\\ncode-davinci-001 34.8‚àí4.263.02.487.23.160.410.269.110.282.46.6\\ncode-davinci-002 47.60.6 78.83.992.70.674.89.082.97.889.02.4\\nTable 6: Pass@ k(%) on the original HumanEval benchmark with Codex models. The numbers in\\norange indicate the absolute improvements of pass@ kon the original benchmark over our modified\\nbenchmark in Table 2.\\nA M ORE IMPLEMENTATION DETAILS\\nWe set the temperature to 0.8, the top pto0.95, the max generation length to 300, and the timeout\\nof executing a test case to 0.1seconds. Specially, for baseline pass@ 1, we use the greedy search\\nsetting with temperature 0. The number of sampling test cases for each problem is set to 100for the\\nHumanEval and MBPP benchmarks, and 50for the APPS and CodeContests benchmarks. When\\nscoring consensus sets in C ODET, we use the square root of |Sx|to reduce the impact caused by code\\nsolutions. A supporting experiment can be found in Appendix C. For code solution post-processing,\\nwe follow Chen et al. (2021) to truncate the generated content by five stop sequences: ‚Äú \\\\nclass ‚Äù,\\n‚Äú\\\\ndef ‚Äù, ‚Äú\\\\n#‚Äù, ‚Äú\\\\nif‚Äù, and ‚Äú \\\\nprint ‚Äù. For the implementation of I NCODER and C ODEGEN, we\\nuse the HuggingFace transformers library (Wolf et al., 2019) and run both models with half preci-\\nsion. In addition, when the number of consensus sets in C ODET is smaller than k, the selection is\\ndone from the highest scoring consensus set to the lowest. When reaching the set with the lowest\\nscore, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets\\nis larger than k, as shown in Figure 6.\\nB R ESULTS ON ORIGINAL HUMAN EVAL\\nAs mentioned in Section 3, for all benchmarks, we remove the example input-output cases from the\\noriginal contexts to avoid exposing real test cases. To study the influence of such modification, we\\ntake HumanEval as an example and perform an additional experiment with its original contexts. The\\nresults are summarized in Table 6. On the one hand, the baseline pass@ 10and pass@ 100results on\\nthe original HumanEval benchmark outperform the modified version, which is reasonable because\\nthe example input-output cases may provide useful information for code generation. Nevertheless,\\nthe pass@ 1results on the original benchmark are basically the same or even worse than the modified\\nversion, suggesting that the Codex models have not fully understood the semantics of the example\\ninput-output cases provided in the contexts. On the other hand, the performance of C ODET is sig-\\nnificantly improved using the original benchmark. This is as expected because the original contexts\\nused for test case generation include real test cases, which could be borrowed by the models during\\nthe generation. Such real test cases will greatly empower C ODET to distinguish correct code solu-\\ntions. Hence, in our experiments, it is indispensable to remove the example input-output cases to\\navoid exposing the real test cases. In this way, the effectiveness of C ODET can be fairly verified.\\nC A NALYSIS ON CODE SOLUTIONS\\nIn C ODET, code solutions that can pass exactly the same test cases are considered consistent in\\nfunctionality and are grouped into the same consensus set. Since we employ top psampling with a\\nrather high temperature of 0.8, the functionality of the code solutions may vary significantly, which\\nresults in more consensus sets. We draw a histogram in Figure 6 to show the number of consensus\\nsets produced by code-cushman-001 and C ODET for each problem on the HumanEval benchmark.\\nThe average and median numbers are 26.8and25.5, respectively. We can find that most problems\\nhave less than 50consensus sets, but the numbers have a high variance among different problems.\\nWe also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in\\nFigure 7. The consensus sets ranked top 1tend to have more code solutions with an average value\\nof9.8, and the numbers also have a high variance.\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=4081, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5668586343920633), NodeWithScore(node=TextNode(id_='0b752686-4982-49af-8767-10196de84c77', embedding=None, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='90b99c45-1691-4cd8-ae53-0c8f732593d7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='e3f991319b58b3204c80f0bce1f2abc8e40b518985fabe35de7031f9cc5260fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3f65451a-f459-4d18-8308-fbb15bc8e031', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3cde2d44b1a7c1fa8c444185cab37c8caa66b7c3892b462f55831a7a84271495')}, text='Published as a conference paper at ICLR 2023\\nk 1 10 50 100 1000 1 2 10 100\\nBaseline C ODET\\nAPPSINTRODUCTORY 29.348.5 60 .9- - 47.318.052.758.49.9 -\\nINTERVIEW 6.4 14.6 25 .4- - 14.37.918.223.38.7 -\\nCOMPETITION 2.5 6.3 14 .5- - 6.23.7 9.8 13.67.3 -\\nCodeContests 1.0 4.1 7 .1 8.8 15 .23.22.2 5.6 9.35.2 12.33.5\\nBaseline Filter C ODET Filter\\nAPPSINTRODUCTORY 43.658.6- - - 49.66.054.359.40.8 -\\nINTERVIEW 13.322.8- - - 16.12.819.524.01.2 -\\nCOMPETITION 7.0 13.3- - - 7.90.9 10.514.10.8 -\\nCodeContests 9.9 14.5 15 .115.2- 9.6‚àí0.311.513.7‚àí0.814.5‚àí0.7\\nTable 9: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nand the one-shot setting. The numbers in red indicate the absolute improvements of C ODET (Filter)\\nover Baseline (Filter) on pass@ 1, pass@ 10and pass@ 100. For C ODET (Filter), temperature is set\\nto0.8and sampling number is set to 50for APPS and 1,000for CodeContests. We do not report\\npass@ 1000 for ‚ÄúBaseline Filter‚Äù because the numbers of code solutions after filtering are less than\\nthe sampling numbers.\\never, as shown in Table 8, after removing a prominent percentage of trivial solutions, there is little\\nperformance gain, which could exactly demonstrate the robustness of C ODET.\\nG R ESULTS ON APPS AND CODECONTESTS IN THE ONE-SHOT SETTING\\nInspired by Chen et al. (2021) and Li et al. (2022b), we build one-shot versions of APPS and Code-\\nContests by appending a single input-output example to the problem description as a formatting\\nhint. After generation, we filter out the generated solutions that cannot pass the given example\\ninput-output cases, which we call the ‚ÄúBaseline Filter‚Äù method. After filtering, we can still perform\\nCODET using the rest of code solutions, called the ‚ÄúC ODET Filter‚Äù method. Following the zero-\\nshot experiments on APPS and CodeContests, we employ code-davinci-002 for generation and set\\nthe sampling number to 50for APPS and 1,000for CodeContests.\\nWe summarize the experimental results in Table 9, where we can find the one-shot performance\\nusing C ODET is much better than that reported in Table 3 in the zero-shot setting. The performance\\nof the baselines can be significantly improved by filtering the solutions with the given example test\\ncases. Moreover, ‚ÄúC ODET Filter‚Äù can further outperform ‚ÄúBaseline Filter‚Äù on the APPS benchmark,\\nespecially for the introductory and interview problems. Nonetheless, for CodeContests and the\\ncompetition level problems in APPS, ‚ÄúC ODET Filter‚Äù has little performance improvement or even\\nperforms slightly worse than ‚ÄúBaseline Filter‚Äù. After manual investigation, we blame such issue to\\nthe generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the\\ninterest of future study on test case generation for more challenging programming problems.\\nH M ORE ANALYSIS ON TESTCASES\\nH.1 S TATISTICS ON TESTCASES\\nHow many valid test cases do the models generate for C ODET? Taking the HumanEval benchmark\\nas an example, we sample 100times for each problem when generating test cases. As illustrated\\nin Figure 2, at each time of sampling, we feed the context calong with an instruction pto the\\nmodel and get the generated content that may contain multiple test cases. Then, as mentioned\\nin Section 4.3, we further post-process the generated samples to get individual test cases that are\\nsyntactically correct.', mimetype='text/plain', start_char_idx=0, end_char_idx=3351, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5604883565453653), NodeWithScore(node=TextNode(id_='4e1c4865-e1e4-401a-a65d-eae76ba400c7', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f61b0c20-6547-41b7-a4cf-4afc6e1a2ac7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e5272d1a-9acf-407d-9ff3-1296fb4123fc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description ‚Äúsum(first index value, last index value)‚Äù is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6541882192111425), NodeWithScore(node=TextNode(id_='acd6646f-4cec-4670-baf6-46681bc72bdf', embedding=None, metadata={'page_label': '1', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0b61254-cdd3-4852-820d-ead70e4148f5', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='8d1d789774d04ee7a7d733e88e2c058730b81bb7f62789f6e367965b7e6eb45e')}, text='Published as a conference paper at ICLR 2023\\nCODET: C ODE GENERATION WITH GENERATED TESTS\\nBei Chen‚àó, Fengji Zhang‚àó, Anh Nguyen‚àó, Daoguang Zan, Zeqi Lin,\\nJian-Guang Lou, Weizhu Chen\\nMicrosoft Corporation\\n{beichen, v-fengjzhang, anhnguyen, v-dazan,\\nzeqi.lin, jlou, wzchen }@microsoft.com\\nABSTRACT\\nThe task of generating code solutions for a given programming problem can bene-\\nfit from the use of pre-trained language models such as Codex, which can produce\\nmultiple diverse samples. However, a major challenge for this task is to select\\nthe most appropriate solution from the multiple samples generated by the pre-\\ntrained language models. A natural way to evaluate the quality and correctness\\nof a code solution is to run it against a set of test cases, but the manual creation\\nof such test cases is often costly and time-consuming. In this paper, we propose\\na novel method, C ODET, that leverages the same pre-trained language models to\\nautomatically generate test cases for the code samples, thus reducing the human\\neffort and increasing the coverage of the test scenarios. C ODET then executes the\\ncode samples using the generated test cases and performs a dual execution agree-\\nment, which considers both the consistency of the outputs against the generated\\ntest cases and the agreement of the outputs with other code samples. We con-\\nduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS,\\nand CodeContests, using five different pre-trained language models with varying\\nsizes and capabilities. Our results show that C ODET can significantly improve the\\nperformance of code solution selection over previous methods, achieving remark-\\nable and consistent gains across different models and benchmarks. For instance,\\nCODET improves the pass@ 1metric on HumanEval to 65.8%, which represents\\nan absolute improvement of 18.8%over the code-davinci-002 model, and an ab-\\nsolute improvement of more than 20% over the previous state-of-the-art results.\\n1 I NTRODUCTION\\nDespite the remarkable progress in pre-training techniques for code generation, selecting a single\\ncorrect solution from multiple candidates generated by large language models remains a hard prob-\\nlem. For instance, Codex (Chen et al., 2021), a state-of-the-art pre-trained language model for code\\ngeneration, can achieve a pass @100 (pass if one or more among 100generated solutions for a given\\nproblem can pass the corresponding test cases) of 77.4%, but a pass @1(correct rate of a single so-\\nlution) of only 33.5%on the HumanEval benchmark (Chen et al., 2021)1. This huge gap limits the\\npractical usefulness of code generation models and motivates us to explore how to pick the correct\\nor best solution from multiple candidates.\\nA straightforward way to verify the correctness of a solution is to execute it and check if it passes\\nall corresponding test cases. This execution-guided approach has been widely adopted in various\\ncode-related tasks, such as code generation (Chen et al., 2021; Li et al., 2022b; Shi et al., 2022),\\ncode translation (Roziere et al., 2021), and program synthesis (Chen et al., 2018; Ellis et al., 2019).\\nHowever, this approach relies heavily on the quality and quantity of test cases, which are often costly\\nand time-consuming to create and maintain. Moreover, in real-world applications like Copilot2, a\\ncode generation tool that assists developers in writing code, it is unrealistic to expect users to provide\\ntest cases for every problem they want to solve. Therefore, we propose to automatically generate\\ntest cases for arbitrary programming problems and use them to quickly verify any solution.\\n‚àóThe first three authors contributed equally.\\n1We report the results on the HumanEval benchmark with the Codex model code-cushman-001. More\\nresults with different models and benchmarks can be found in Section 4.1 and 4.2\\n2https://github.com/features/copilot\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=3872, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6521034157719083), NodeWithScore(node=TextNode(id_='195f22c9-84bc-4fb0-bd86-9180c2d5e90a', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='71b21d63-660e-4bc1-a6d7-76cb06fa58bc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f2ebe392-8f46-4969-b5e3-1ad135b59841', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6909084127555096), NodeWithScore(node=TextNode(id_='f2ebe392-8f46-4969-b5e3-1ad135b59841', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='71b21d63-660e-4bc1-a6d7-76cb06fa58bc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='195f22c9-84bc-4fb0-bd86-9180c2d5e90a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6865730581017966)], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"\"\"\n",
    "    Answer the following questions about CodeT:\n",
    "    - What is the problem CodeT is solving?\n",
    "    - Why is solving this problem important?\n",
    "    - What is CodeT's contribution?\n",
    "    - Explain CodeT method\n",
    "    - How does CodeT perform compared to other frameworks?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 9\n",
      "file_name: codet_avaliacao.pdf\n",
      "file_path: data\\codet_avaliacao.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1272933\n",
      "creation_date: 2024-10-29\n",
      "last_modified_date: 2024-10-29\n",
      "\n",
      "Published as a conference paper at ICLR 2023\n",
      "of import statements, while the remaining problems are attributed to the failure of the model to\n",
      "understand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\n",
      "correct understanding of the description ‚Äúsum(first index value, last index value)‚Äù is to add the first\n",
      "and last values, while the code solutions that sum all values from the first to the last are ranked top\n",
      "1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\n",
      "for future studies on improving code generation for more difficult programming problems.\n",
      "5 R ELATED WORK\n",
      "Code Generation with Large Models Recently, a number of large pre-trained language mod-\n",
      "els have been proposed for code generation. Benefiting from billions of trainable parameters and\n",
      "massive publicly available source code, models could achieve surprisingly good performance. For\n",
      "instance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\n",
      "tors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\n",
      "to provide real-time coding suggestions. Other open-source code generation models include GPT-\n",
      "Neo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\n",
      "PolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\n",
      "In our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\n",
      "competitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\n",
      "Automatic Test Case Generation Automated test case generation for programming problems\n",
      "can reduce the effort of writing test cases manually by developers. Early works including Ran-\n",
      "doop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\n",
      "DynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\n",
      "generate test cases for statically typed programming languages like Java. The later proposed Pyn-\n",
      "guin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\n",
      "theless, they are all search-based heuristics methods, which have limitations to the diversity and\n",
      "quantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\n",
      "et al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\n",
      "and T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\n",
      "works that require heuristic rules or model training, we directly sample test cases from powerful\n",
      "code generation models like Codex in the zero-shot setting with elaborate prompts.\n",
      "Code Selection from Multiple Samples Despite large models have achieved great performance in\n",
      "code generation, the models need to sample many times to find the correct answer. Recently, several\n",
      "approaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\n",
      "et al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\n",
      "to jointly train the generator and ranker through a multi-task framework. In the domain of general\n",
      "purpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\n",
      "been proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\n",
      "Lahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\n",
      "user interactions, we let the large models generate test cases for themselves and automatically rank\n",
      "the solutions based on the test-driven dual execution agreement. The idea of ranking based on\n",
      "agreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\n",
      "6 C ONCLUSION AND FUTURE WORK\n",
      "In this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\n",
      "language models to generate both the code solutions and the test cases. C ODET executes the code\n",
      "solutions using the test cases and chooses the best solution based on the dual execution agreement.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
