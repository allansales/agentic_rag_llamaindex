{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Llama 3\n",
    "llm = Groq(model=\"llama-3.1-70b-versatile\")\n",
    "Settings.llm = llm\n",
    "\n",
    "def get_doc_tools(file_path: str) -> str: \n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "    # summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # summary_query_engine = summary_index.as_query_engine(\n",
    "    #     response_mode=\"tree_summarize\",\n",
    "    #     use_async=True,\n",
    "    # )\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "    # summary_tool = QueryEngineTool.from_defaults(\n",
    "    #     query_engine=summary_query_engine,\n",
    "    #     description=(\n",
    "    #         \"Useful for summarization questions related to paul_graham_essay\"\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Useful for retrieving specific context from the paul_graham_essay.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # return(vector_tool, summary_tool)\n",
    "    return(vector_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool = get_doc_tools(\"data/codet_avaliacao.pdf\")\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Answer the following questions about CodeT:\n",
      "    - What is the problem CodeT is solving?\n",
      "    - Why is solving this problem important?\n",
      "    - What is CodeT's contribution?\n",
      "    - Explain CodeT method\n",
      "    - How does CodeT perform compared to other frameworks?\n",
      "    \n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is the problem CodeT is solving?\"}\n",
      "=== Function Output ===\n",
      "CodeT is solving the problem of code generation and test case generation for programming problems, with the goal of improving the accuracy and efficiency of code generation models.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Why is solving the problem CodeT is addressing important?\"}\n",
      "=== Function Output ===\n",
      "Solving the problem that CodeT is addressing is important because it can improve the performance of code generation models in solving difficult programming problems. The problem of code generation is significant as it can automate the process of writing code, reducing the effort required by developers. However, current models often struggle with understanding problem descriptions and generating correct code solutions. By addressing this problem, CodeT can help improve the accuracy and reliability of code generation models, making them more useful in real-world applications.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"What is CodeT's contribution?\"}\n",
      "=== Function Output ===\n",
      "CodeT's contribution is that it can significantly improve the performance of code generation models by distinguishing correct code solutions, especially when the models have not fully understood the semantics of the example input-output cases provided in the contexts.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain CodeT method?\"}\n",
      "=== Function Output ===\n",
      "CodeT method is based on test-driven execution agreement, which takes test case information into consideration. It uses a dual execution agreement approach to improve the performance of pre-trained language models. The method involves generating code solutions and test cases, and then evaluating the code solutions based on the test cases. The results show that CodeT can significantly improve the pass@1, with absolute improvements in the range of 4.2% to 13.1%.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"How does CodeT perform compared to other frameworks?\"}\n",
      "=== Function Output ===\n",
      "CodeT consistently outperforms other frameworks, such as AlphaCode-C, on various benchmarks, including HumanEval, MBPP, APPS, and CodeContests. It achieves significant improvements in pass@1, with absolute improvements ranging from 4.2% to 13.1% on different models, including code-davinci-002, I NCODER-6B, and C ODEGEN-MONO-16B. Additionally, CodeT's performance is superior to the baseline and other models, with pass@2 results close to the baseline pass@10 results.\n",
      "=== LLM Response ===\n",
      "CodeT is solving the problem of code generation and test case generation for programming problems, with the goal of improving the accuracy and efficiency of code generation models. Solving the problem that CodeT is addressing is important because it can improve the performance of code generation models in solving difficult programming problems. CodeT's contribution is that it can significantly improve the performance of code generation models by distinguishing correct code solutions, especially when the models have not fully understood the semantics of the example input-output cases provided in the contexts. CodeT method is based on test-driven execution agreement, which takes test case information into consideration. It uses a dual execution agreement approach to improve the performance of pre-trained language models. CodeT consistently outperforms other frameworks, such as AlphaCode-C, on various benchmarks, including HumanEval, MBPP, APPS, and CodeContests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"CodeT is solving the problem of code generation and test case generation for programming problems, with the goal of improving the accuracy and efficiency of code generation models. Solving the problem that CodeT is addressing is important because it can improve the performance of code generation models in solving difficult programming problems. CodeT's contribution is that it can significantly improve the performance of code generation models by distinguishing correct code solutions, especially when the models have not fully understood the semantics of the example input-output cases provided in the contexts. CodeT method is based on test-driven execution agreement, which takes test case information into consideration. It uses a dual execution agreement approach to improve the performance of pre-trained language models. CodeT consistently outperforms other frameworks, such as AlphaCode-C, on various benchmarks, including HumanEval, MBPP, APPS, and CodeContests.\", sources=[ToolOutput(content='CodeT is solving the problem of code generation and test case generation for programming problems, with the goal of improving the accuracy and efficiency of code generation models.', tool_name='query_engine_tool', raw_input={'input': 'What is the problem CodeT is solving?'}, raw_output=Response(response='CodeT is solving the problem of code generation and test case generation for programming problems, with the goal of improving the accuracy and efficiency of code generation models.', source_nodes=[NodeWithScore(node=TextNode(id_='b9225e75-5d8a-4b1c-b52e-0ae33bf63758', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62024b73-b68c-42f8-a512-711797ad55c9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80824aa6-e182-463e-93e1-c774479e3662', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description “sum(first index value, last index value)” is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6393933025106274), NodeWithScore(node=TextNode(id_='bc5bac15-87d4-4502-a1e4-ad1900836a70', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0666ced-b146-4ed2-869b-16d9af0a1237', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='041eb174-afd0-472b-a96e-e946ad3b6367', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0𝑥1\\n𝑥2\\n𝑥3𝑦1\\n𝑦2\\n𝑦3\\n𝑦4\\n𝑦5 returna 𝑥4\\nFigure 3: A simple example of the programming\\nproblem “return the square of a number”. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x∈ S x, y∈ S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n• We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ˆxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x∈X, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,···,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x∈ Sx, y∈ Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6348175268840989)], metadata={'b9225e75-5d8a-4b1c-b52e-0ae33bf63758': {'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'bc5bac15-87d4-4502-a1e4-ad1900836a70': {'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content='Solving the problem that CodeT is addressing is important because it can improve the performance of code generation models in solving difficult programming problems. The problem of code generation is significant as it can automate the process of writing code, reducing the effort required by developers. However, current models often struggle with understanding problem descriptions and generating correct code solutions. By addressing this problem, CodeT can help improve the accuracy and reliability of code generation models, making them more useful in real-world applications.', tool_name='query_engine_tool', raw_input={'input': 'Why is solving the problem CodeT is addressing important?'}, raw_output=Response(response='Solving the problem that CodeT is addressing is important because it can improve the performance of code generation models in solving difficult programming problems. The problem of code generation is significant as it can automate the process of writing code, reducing the effort required by developers. However, current models often struggle with understanding problem descriptions and generating correct code solutions. By addressing this problem, CodeT can help improve the accuracy and reliability of code generation models, making them more useful in real-world applications.', source_nodes=[NodeWithScore(node=TextNode(id_='b9225e75-5d8a-4b1c-b52e-0ae33bf63758', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62024b73-b68c-42f8-a512-711797ad55c9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80824aa6-e182-463e-93e1-c774479e3662', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description “sum(first index value, last index value)” is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5963480731751994), NodeWithScore(node=TextNode(id_='bc5bac15-87d4-4502-a1e4-ad1900836a70', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0666ced-b146-4ed2-869b-16d9af0a1237', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='041eb174-afd0-472b-a96e-e946ad3b6367', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0𝑥1\\n𝑥2\\n𝑥3𝑦1\\n𝑦2\\n𝑦3\\n𝑦4\\n𝑦5 returna 𝑥4\\nFigure 3: A simple example of the programming\\nproblem “return the square of a number”. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x∈ S x, y∈ S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n• We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ˆxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x∈X, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,···,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x∈ Sx, y∈ Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5831006007311904)], metadata={'b9225e75-5d8a-4b1c-b52e-0ae33bf63758': {'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'bc5bac15-87d4-4502-a1e4-ad1900836a70': {'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content=\"CodeT's contribution is that it can significantly improve the performance of code generation models by distinguishing correct code solutions, especially when the models have not fully understood the semantics of the example input-output cases provided in the contexts.\", tool_name='query_engine_tool', raw_input={'input': \"What is CodeT's contribution?\"}, raw_output=Response(response=\"CodeT's contribution is that it can significantly improve the performance of code generation models by distinguishing correct code solutions, especially when the models have not fully understood the semantics of the example input-output cases provided in the contexts.\", source_nodes=[NodeWithScore(node=TextNode(id_='a36e6f54-9f87-458b-97b1-710233e47b05', embedding=None, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b3d0998b-f414-4ef3-9c30-5235524a6b08', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='91da99ff59e8f9d8547e5c131dc76afe5ef0c57ba8e0c220cb8a1a6cf6aab276')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 100 1 2 10\\ncode-cushman-001 31.7−1.856.42.184.16.758.614.165.715.680.114.4\\ncode-davinci-001 34.8−4.263.02.487.23.160.410.269.110.282.46.6\\ncode-davinci-002 47.60.6 78.83.992.70.674.89.082.97.889.02.4\\nTable 6: Pass@ k(%) on the original HumanEval benchmark with Codex models. The numbers in\\norange indicate the absolute improvements of pass@ kon the original benchmark over our modified\\nbenchmark in Table 2.\\nA M ORE IMPLEMENTATION DETAILS\\nWe set the temperature to 0.8, the top pto0.95, the max generation length to 300, and the timeout\\nof executing a test case to 0.1seconds. Specially, for baseline pass@ 1, we use the greedy search\\nsetting with temperature 0. The number of sampling test cases for each problem is set to 100for the\\nHumanEval and MBPP benchmarks, and 50for the APPS and CodeContests benchmarks. When\\nscoring consensus sets in C ODET, we use the square root of |Sx|to reduce the impact caused by code\\nsolutions. A supporting experiment can be found in Appendix C. For code solution post-processing,\\nwe follow Chen et al. (2021) to truncate the generated content by five stop sequences: “ \\\\nclass ”,\\n“\\\\ndef ”, “\\\\n#”, “\\\\nif”, and “ \\\\nprint ”. For the implementation of I NCODER and C ODEGEN, we\\nuse the HuggingFace transformers library (Wolf et al., 2019) and run both models with half preci-\\nsion. In addition, when the number of consensus sets in C ODET is smaller than k, the selection is\\ndone from the highest scoring consensus set to the lowest. When reaching the set with the lowest\\nscore, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets\\nis larger than k, as shown in Figure 6.\\nB R ESULTS ON ORIGINAL HUMAN EVAL\\nAs mentioned in Section 3, for all benchmarks, we remove the example input-output cases from the\\noriginal contexts to avoid exposing real test cases. To study the influence of such modification, we\\ntake HumanEval as an example and perform an additional experiment with its original contexts. The\\nresults are summarized in Table 6. On the one hand, the baseline pass@ 10and pass@ 100results on\\nthe original HumanEval benchmark outperform the modified version, which is reasonable because\\nthe example input-output cases may provide useful information for code generation. Nevertheless,\\nthe pass@ 1results on the original benchmark are basically the same or even worse than the modified\\nversion, suggesting that the Codex models have not fully understood the semantics of the example\\ninput-output cases provided in the contexts. On the other hand, the performance of C ODET is sig-\\nnificantly improved using the original benchmark. This is as expected because the original contexts\\nused for test case generation include real test cases, which could be borrowed by the models during\\nthe generation. Such real test cases will greatly empower C ODET to distinguish correct code solu-\\ntions. Hence, in our experiments, it is indispensable to remove the example input-output cases to\\navoid exposing the real test cases. In this way, the effectiveness of C ODET can be fairly verified.\\nC A NALYSIS ON CODE SOLUTIONS\\nIn C ODET, code solutions that can pass exactly the same test cases are considered consistent in\\nfunctionality and are grouped into the same consensus set. Since we employ top psampling with a\\nrather high temperature of 0.8, the functionality of the code solutions may vary significantly, which\\nresults in more consensus sets. We draw a histogram in Figure 6 to show the number of consensus\\nsets produced by code-cushman-001 and C ODET for each problem on the HumanEval benchmark.\\nThe average and median numbers are 26.8and25.5, respectively. We can find that most problems\\nhave less than 50consensus sets, but the numbers have a high variance among different problems.\\nWe also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in\\nFigure 7. The consensus sets ranked top 1tend to have more code solutions with an average value\\nof9.8, and the numbers also have a high variance.\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=4081, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5668586343920633), NodeWithScore(node=TextNode(id_='1d51a8ed-f726-4cb3-8017-caac1e51ee8c', embedding=None, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1c820891-bb4a-4106-8782-c733ec5c4a1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='e3f991319b58b3204c80f0bce1f2abc8e40b518985fabe35de7031f9cc5260fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a6b1eb76-c0c4-42b6-b48a-ebb1b77559a4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3cde2d44b1a7c1fa8c444185cab37c8caa66b7c3892b462f55831a7a84271495')}, text='Published as a conference paper at ICLR 2023\\nk 1 10 50 100 1000 1 2 10 100\\nBaseline C ODET\\nAPPSINTRODUCTORY 29.348.5 60 .9- - 47.318.052.758.49.9 -\\nINTERVIEW 6.4 14.6 25 .4- - 14.37.918.223.38.7 -\\nCOMPETITION 2.5 6.3 14 .5- - 6.23.7 9.8 13.67.3 -\\nCodeContests 1.0 4.1 7 .1 8.8 15 .23.22.2 5.6 9.35.2 12.33.5\\nBaseline Filter C ODET Filter\\nAPPSINTRODUCTORY 43.658.6- - - 49.66.054.359.40.8 -\\nINTERVIEW 13.322.8- - - 16.12.819.524.01.2 -\\nCOMPETITION 7.0 13.3- - - 7.90.9 10.514.10.8 -\\nCodeContests 9.9 14.5 15 .115.2- 9.6−0.311.513.7−0.814.5−0.7\\nTable 9: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nand the one-shot setting. The numbers in red indicate the absolute improvements of C ODET (Filter)\\nover Baseline (Filter) on pass@ 1, pass@ 10and pass@ 100. For C ODET (Filter), temperature is set\\nto0.8and sampling number is set to 50for APPS and 1,000for CodeContests. We do not report\\npass@ 1000 for “Baseline Filter” because the numbers of code solutions after filtering are less than\\nthe sampling numbers.\\never, as shown in Table 8, after removing a prominent percentage of trivial solutions, there is little\\nperformance gain, which could exactly demonstrate the robustness of C ODET.\\nG R ESULTS ON APPS AND CODECONTESTS IN THE ONE-SHOT SETTING\\nInspired by Chen et al. (2021) and Li et al. (2022b), we build one-shot versions of APPS and Code-\\nContests by appending a single input-output example to the problem description as a formatting\\nhint. After generation, we filter out the generated solutions that cannot pass the given example\\ninput-output cases, which we call the “Baseline Filter” method. After filtering, we can still perform\\nCODET using the rest of code solutions, called the “C ODET Filter” method. Following the zero-\\nshot experiments on APPS and CodeContests, we employ code-davinci-002 for generation and set\\nthe sampling number to 50for APPS and 1,000for CodeContests.\\nWe summarize the experimental results in Table 9, where we can find the one-shot performance\\nusing C ODET is much better than that reported in Table 3 in the zero-shot setting. The performance\\nof the baselines can be significantly improved by filtering the solutions with the given example test\\ncases. Moreover, “C ODET Filter” can further outperform “Baseline Filter” on the APPS benchmark,\\nespecially for the introductory and interview problems. Nonetheless, for CodeContests and the\\ncompetition level problems in APPS, “C ODET Filter” has little performance improvement or even\\nperforms slightly worse than “Baseline Filter”. After manual investigation, we blame such issue to\\nthe generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the\\ninterest of future study on test case generation for more challenging programming problems.\\nH M ORE ANALYSIS ON TESTCASES\\nH.1 S TATISTICS ON TESTCASES\\nHow many valid test cases do the models generate for C ODET? Taking the HumanEval benchmark\\nas an example, we sample 100times for each problem when generating test cases. As illustrated\\nin Figure 2, at each time of sampling, we feed the context calong with an instruction pto the\\nmodel and get the generated content that may contain multiple test cases. Then, as mentioned\\nin Section 4.3, we further post-process the generated samples to get individual test cases that are\\nsyntactically correct.', mimetype='text/plain', start_char_idx=0, end_char_idx=3351, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5604883565453653)], metadata={'a36e6f54-9f87-458b-97b1-710233e47b05': {'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, '1d51a8ed-f726-4cb3-8017-caac1e51ee8c': {'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content='CodeT method is based on test-driven execution agreement, which takes test case information into consideration. It uses a dual execution agreement approach to improve the performance of pre-trained language models. The method involves generating code solutions and test cases, and then evaluating the code solutions based on the test cases. The results show that CodeT can significantly improve the pass@1, with absolute improvements in the range of 4.2% to 13.1%.', tool_name='query_engine_tool', raw_input={'input': 'Explain CodeT method?'}, raw_output=Response(response='CodeT method is based on test-driven execution agreement, which takes test case information into consideration. It uses a dual execution agreement approach to improve the performance of pre-trained language models. The method involves generating code solutions and test cases, and then evaluating the code solutions based on the test cases. The results show that CodeT can significantly improve the pass@1, with absolute improvements in the range of 4.2% to 13.1%.', source_nodes=[NodeWithScore(node=TextNode(id_='9662bad8-69f3-449a-90b2-8504b4b939a4', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6514327296887347), NodeWithScore(node=TextNode(id_='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9662bad8-69f3-449a-90b2-8504b4b939a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6501219449053354)], metadata={'9662bad8-69f3-449a-90b2-8504b4b939a4': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, 'd1eeefea-9d44-4c9a-9758-52c2d23a44f0': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False), ToolOutput(content=\"CodeT consistently outperforms other frameworks, such as AlphaCode-C, on various benchmarks, including HumanEval, MBPP, APPS, and CodeContests. It achieves significant improvements in pass@1, with absolute improvements ranging from 4.2% to 13.1% on different models, including code-davinci-002, I NCODER-6B, and C ODEGEN-MONO-16B. Additionally, CodeT's performance is superior to the baseline and other models, with pass@2 results close to the baseline pass@10 results.\", tool_name='query_engine_tool', raw_input={'input': 'How does CodeT perform compared to other frameworks?'}, raw_output=Response(response=\"CodeT consistently outperforms other frameworks, such as AlphaCode-C, on various benchmarks, including HumanEval, MBPP, APPS, and CodeContests. It achieves significant improvements in pass@1, with absolute improvements ranging from 4.2% to 13.1% on different models, including code-davinci-002, I NCODER-6B, and C ODEGEN-MONO-16B. Additionally, CodeT's performance is superior to the baseline and other models, with pass@2 results close to the baseline pass@10 results.\", source_nodes=[NodeWithScore(node=TextNode(id_='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9662bad8-69f3-449a-90b2-8504b4b939a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6909084127555096), NodeWithScore(node=TextNode(id_='9662bad8-69f3-449a-90b2-8504b4b939a4', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6865730581017966)], metadata={'d1eeefea-9d44-4c9a-9758-52c2d23a44f0': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, '9662bad8-69f3-449a-90b2-8504b4b939a4': {'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}}), is_error=False)], source_nodes=[NodeWithScore(node=TextNode(id_='b9225e75-5d8a-4b1c-b52e-0ae33bf63758', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62024b73-b68c-42f8-a512-711797ad55c9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80824aa6-e182-463e-93e1-c774479e3662', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description “sum(first index value, last index value)” is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6393933025106274), NodeWithScore(node=TextNode(id_='bc5bac15-87d4-4502-a1e4-ad1900836a70', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0666ced-b146-4ed2-869b-16d9af0a1237', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='041eb174-afd0-472b-a96e-e946ad3b6367', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0𝑥1\\n𝑥2\\n𝑥3𝑦1\\n𝑦2\\n𝑦3\\n𝑦4\\n𝑦5 returna 𝑥4\\nFigure 3: A simple example of the programming\\nproblem “return the square of a number”. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x∈ S x, y∈ S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n• We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ˆxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x∈X, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,···,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x∈ Sx, y∈ Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6348175268840989), NodeWithScore(node=TextNode(id_='b9225e75-5d8a-4b1c-b52e-0ae33bf63758', embedding=None, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62024b73-b68c-42f8-a512-711797ad55c9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='f11d6bbded44845ee0f345d12d16a06bd89f514dc712fd3461b855ee0ccc8416'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80824aa6-e182-463e-93e1-c774479e3662', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c7102193dc11ef7b32cb764fc98d5ea90081943b84cd2e3481b5a949f8e2573e')}, text='Published as a conference paper at ICLR 2023\\nof import statements, while the remaining problems are attributed to the failure of the model to\\nunderstand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\\ncorrect understanding of the description “sum(first index value, last index value)” is to add the first\\nand last values, while the code solutions that sum all values from the first to the last are ranked top\\n1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\\nfor future studies on improving code generation for more difficult programming problems.\\n5 R ELATED WORK\\nCode Generation with Large Models Recently, a number of large pre-trained language mod-\\nels have been proposed for code generation. Benefiting from billions of trainable parameters and\\nmassive publicly available source code, models could achieve surprisingly good performance. For\\ninstance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\\ntors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\\nto provide real-time coding suggestions. Other open-source code generation models include GPT-\\nNeo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\\nPolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\\nIn our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\\ncompetitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\\nAutomatic Test Case Generation Automated test case generation for programming problems\\ncan reduce the effort of writing test cases manually by developers. Early works including Ran-\\ndoop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\\nDynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\\ngenerate test cases for statically typed programming languages like Java. The later proposed Pyn-\\nguin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\\ntheless, they are all search-based heuristics methods, which have limitations to the diversity and\\nquantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\\net al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\\nand T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\\nworks that require heuristic rules or model training, we directly sample test cases from powerful\\ncode generation models like Codex in the zero-shot setting with elaborate prompts.\\nCode Selection from Multiple Samples Despite large models have achieved great performance in\\ncode generation, the models need to sample many times to find the correct answer. Recently, several\\napproaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\\net al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\\nto jointly train the generator and ranker through a multi-task framework. In the domain of general\\npurpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\\nbeen proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\\nLahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\\nuser interactions, we let the large models generate test cases for themselves and automatically rank\\nthe solutions based on the test-driven dual execution agreement. The idea of ranking based on\\nagreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\\n6 C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\\nlanguage models to generate both the code solutions and the test cases. C ODET executes the code\\nsolutions using the test cases and chooses the best solution based on the dual execution agreement.', mimetype='text/plain', start_char_idx=0, end_char_idx=4136, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5963480731751994), NodeWithScore(node=TextNode(id_='bc5bac15-87d4-4502-a1e4-ad1900836a70', embedding=None, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c0666ced-b146-4ed2-869b-16d9af0a1237', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='92b18f0624460036b55e7d9b23d8e01756ba85a19f3bb05151a1501baf7af84d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='041eb174-afd0-472b-a96e-e946ad3b6367', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d41c8ffc5c6f8e9cb516bc5d1bc6d8ac50b69bc80e2a0245bfca167409c2431')}, text='Published as a conference paper at ICLR 2023\\nELT layout\\nCode Solutionsreturna**2\\nreturna*a\\nreturna*2\\nTest Casesassertnum_square (1) == 1\\nassertnum_square (2) == 4\\nassertnum_square (1) == 2\\nassertnum_square (3) == 6assertnum_square (0) == 0𝑥1\\n𝑥2\\n𝑥3𝑦1\\n𝑦2\\n𝑦3\\n𝑦4\\n𝑦5 returna 𝑥4\\nFigure 3: A simple example of the programming\\nproblem “return the square of a number”. The\\ngray line between xandyindicates that xcan\\npassy, i.e., (x, y)is a hypothetical inlier. The\\ngreen or purple box indicates a consensus set.Benchmark Problems GT Tests n\\nHumanEval 164 7 .77 100\\nMBPP 427 3 .1 100\\nAPPSINTRODUCTORY 1,000\\n20.99 50 INTERVIEW 3,000\\nCOMPETITION 1,000\\nCodeContests 165 203 .7 1 ,000\\nTable 1: Statistics of benchmarks: the total num-\\nber of problems in the benchmark ( Problems ),\\nthe average number of ground-truth test cases per\\nproblem ( GT Tests ), and the number of sampling\\ncode solutions for each problem ( n).\\nsensus set is the set of all pairs that consist of a code solution from Sxand a test case\\nfromSy, i.e.,S={(x, y)|x∈ S x, y∈ S y}. For example in Figure 3, we can get\\nSx={x1, x2},Sy={y1, y2, y3}from the hypothetical inlier (x1, y1)(shown in green\\nbox), and Sx={x3},Sy={y2, y3, y4, y5}from (x3, y2)(shown in purple box).\\n• We score the consensus set as f(S) =|Sx||Sy|, where |Sx|is the number of code solutions\\ninSxand|Sy|is the number of test cases in Sy. This score is equal to the number of pairs\\nin the consensus set. The intuition is that the more pairs that agree with the hypothetical\\nfunctionality, the more likely this functionality is correct, according to our assumptions.\\nFollowing the example in Figure 3, the consensus set scores are 6and4for the hypothetical\\ninliers (x1, y1)and(x3, y2), respectively.\\nWe repeat the above procedure for a fixed number of times, each time producing a consensus set with\\nits score. Finally, we get the best code solution ˆxby selecting any code solution from the consensus\\nset with the highest score. If we want to obtain kcode solutions, we can select the top kconsensus\\nsets with the highest scores, and one code solution is picked up from each of the kconsensus sets.\\nIn practice, when the number of code solutions in Dis not large, we can simplify the above method\\nby examining all possible pairs in D, instead of sampling pairs from D. Specially, for each code\\nsolution x∈X, we run it with every test case in Yand keep track of which test cases it passes. We\\ngroup together code solutions that pass the same test cases, because they have the same functionality.\\nThis way, we divide all code solutions in Xinto groups based on their functionality, which we write\\nasX={S1\\nx,S2\\nx,···,SK\\nx}, where Kis the number of code solution groups. Each group Sxhas a set\\nof test cases that it passes, which we write as Sy. Then, we get Kconsensus sets, each of which has\\nthe form S={(x, y)|x∈ Sx, y∈ Sy}. We can score each consensus set by f(S) =|Sx||Sy|, as\\nbefore. This naive version captures the same underline intuition, but it finds all consensus sets right\\naway, without sampling pairs repeatedly.\\n3 E XPERIMENTAL SETUP\\nModels Our experiments are based on Codex (Chen et al., 2021), I NCODER (Fried et al., 2022)\\nand C ODEGEN(Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and\\nproficient in understanding the provided context and generating functional programs.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5831006007311904), NodeWithScore(node=TextNode(id_='a36e6f54-9f87-458b-97b1-710233e47b05', embedding=None, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b3d0998b-f414-4ef3-9c30-5235524a6b08', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='91da99ff59e8f9d8547e5c131dc76afe5ef0c57ba8e0c220cb8a1a6cf6aab276')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 100 1 2 10\\ncode-cushman-001 31.7−1.856.42.184.16.758.614.165.715.680.114.4\\ncode-davinci-001 34.8−4.263.02.487.23.160.410.269.110.282.46.6\\ncode-davinci-002 47.60.6 78.83.992.70.674.89.082.97.889.02.4\\nTable 6: Pass@ k(%) on the original HumanEval benchmark with Codex models. The numbers in\\norange indicate the absolute improvements of pass@ kon the original benchmark over our modified\\nbenchmark in Table 2.\\nA M ORE IMPLEMENTATION DETAILS\\nWe set the temperature to 0.8, the top pto0.95, the max generation length to 300, and the timeout\\nof executing a test case to 0.1seconds. Specially, for baseline pass@ 1, we use the greedy search\\nsetting with temperature 0. The number of sampling test cases for each problem is set to 100for the\\nHumanEval and MBPP benchmarks, and 50for the APPS and CodeContests benchmarks. When\\nscoring consensus sets in C ODET, we use the square root of |Sx|to reduce the impact caused by code\\nsolutions. A supporting experiment can be found in Appendix C. For code solution post-processing,\\nwe follow Chen et al. (2021) to truncate the generated content by five stop sequences: “ \\\\nclass ”,\\n“\\\\ndef ”, “\\\\n#”, “\\\\nif”, and “ \\\\nprint ”. For the implementation of I NCODER and C ODEGEN, we\\nuse the HuggingFace transformers library (Wolf et al., 2019) and run both models with half preci-\\nsion. In addition, when the number of consensus sets in C ODET is smaller than k, the selection is\\ndone from the highest scoring consensus set to the lowest. When reaching the set with the lowest\\nscore, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets\\nis larger than k, as shown in Figure 6.\\nB R ESULTS ON ORIGINAL HUMAN EVAL\\nAs mentioned in Section 3, for all benchmarks, we remove the example input-output cases from the\\noriginal contexts to avoid exposing real test cases. To study the influence of such modification, we\\ntake HumanEval as an example and perform an additional experiment with its original contexts. The\\nresults are summarized in Table 6. On the one hand, the baseline pass@ 10and pass@ 100results on\\nthe original HumanEval benchmark outperform the modified version, which is reasonable because\\nthe example input-output cases may provide useful information for code generation. Nevertheless,\\nthe pass@ 1results on the original benchmark are basically the same or even worse than the modified\\nversion, suggesting that the Codex models have not fully understood the semantics of the example\\ninput-output cases provided in the contexts. On the other hand, the performance of C ODET is sig-\\nnificantly improved using the original benchmark. This is as expected because the original contexts\\nused for test case generation include real test cases, which could be borrowed by the models during\\nthe generation. Such real test cases will greatly empower C ODET to distinguish correct code solu-\\ntions. Hence, in our experiments, it is indispensable to remove the example input-output cases to\\navoid exposing the real test cases. In this way, the effectiveness of C ODET can be fairly verified.\\nC A NALYSIS ON CODE SOLUTIONS\\nIn C ODET, code solutions that can pass exactly the same test cases are considered consistent in\\nfunctionality and are grouped into the same consensus set. Since we employ top psampling with a\\nrather high temperature of 0.8, the functionality of the code solutions may vary significantly, which\\nresults in more consensus sets. We draw a histogram in Figure 6 to show the number of consensus\\nsets produced by code-cushman-001 and C ODET for each problem on the HumanEval benchmark.\\nThe average and median numbers are 26.8and25.5, respectively. We can find that most problems\\nhave less than 50consensus sets, but the numbers have a high variance among different problems.\\nWe also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in\\nFigure 7. The consensus sets ranked top 1tend to have more code solutions with an average value\\nof9.8, and the numbers also have a high variance.\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=4081, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5668586343920633), NodeWithScore(node=TextNode(id_='1d51a8ed-f726-4cb3-8017-caac1e51ee8c', embedding=None, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1c820891-bb4a-4106-8782-c733ec5c4a1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='e3f991319b58b3204c80f0bce1f2abc8e40b518985fabe35de7031f9cc5260fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a6b1eb76-c0c4-42b6-b48a-ebb1b77559a4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3cde2d44b1a7c1fa8c444185cab37c8caa66b7c3892b462f55831a7a84271495')}, text='Published as a conference paper at ICLR 2023\\nk 1 10 50 100 1000 1 2 10 100\\nBaseline C ODET\\nAPPSINTRODUCTORY 29.348.5 60 .9- - 47.318.052.758.49.9 -\\nINTERVIEW 6.4 14.6 25 .4- - 14.37.918.223.38.7 -\\nCOMPETITION 2.5 6.3 14 .5- - 6.23.7 9.8 13.67.3 -\\nCodeContests 1.0 4.1 7 .1 8.8 15 .23.22.2 5.6 9.35.2 12.33.5\\nBaseline Filter C ODET Filter\\nAPPSINTRODUCTORY 43.658.6- - - 49.66.054.359.40.8 -\\nINTERVIEW 13.322.8- - - 16.12.819.524.01.2 -\\nCOMPETITION 7.0 13.3- - - 7.90.9 10.514.10.8 -\\nCodeContests 9.9 14.5 15 .115.2- 9.6−0.311.513.7−0.814.5−0.7\\nTable 9: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nand the one-shot setting. The numbers in red indicate the absolute improvements of C ODET (Filter)\\nover Baseline (Filter) on pass@ 1, pass@ 10and pass@ 100. For C ODET (Filter), temperature is set\\nto0.8and sampling number is set to 50for APPS and 1,000for CodeContests. We do not report\\npass@ 1000 for “Baseline Filter” because the numbers of code solutions after filtering are less than\\nthe sampling numbers.\\never, as shown in Table 8, after removing a prominent percentage of trivial solutions, there is little\\nperformance gain, which could exactly demonstrate the robustness of C ODET.\\nG R ESULTS ON APPS AND CODECONTESTS IN THE ONE-SHOT SETTING\\nInspired by Chen et al. (2021) and Li et al. (2022b), we build one-shot versions of APPS and Code-\\nContests by appending a single input-output example to the problem description as a formatting\\nhint. After generation, we filter out the generated solutions that cannot pass the given example\\ninput-output cases, which we call the “Baseline Filter” method. After filtering, we can still perform\\nCODET using the rest of code solutions, called the “C ODET Filter” method. Following the zero-\\nshot experiments on APPS and CodeContests, we employ code-davinci-002 for generation and set\\nthe sampling number to 50for APPS and 1,000for CodeContests.\\nWe summarize the experimental results in Table 9, where we can find the one-shot performance\\nusing C ODET is much better than that reported in Table 3 in the zero-shot setting. The performance\\nof the baselines can be significantly improved by filtering the solutions with the given example test\\ncases. Moreover, “C ODET Filter” can further outperform “Baseline Filter” on the APPS benchmark,\\nespecially for the introductory and interview problems. Nonetheless, for CodeContests and the\\ncompetition level problems in APPS, “C ODET Filter” has little performance improvement or even\\nperforms slightly worse than “Baseline Filter”. After manual investigation, we blame such issue to\\nthe generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the\\ninterest of future study on test case generation for more challenging programming problems.\\nH M ORE ANALYSIS ON TESTCASES\\nH.1 S TATISTICS ON TESTCASES\\nHow many valid test cases do the models generate for C ODET? Taking the HumanEval benchmark\\nas an example, we sample 100times for each problem when generating test cases. As illustrated\\nin Figure 2, at each time of sampling, we feed the context calong with an instruction pto the\\nmodel and get the generated content that may contain multiple test cases. Then, as mentioned\\nin Section 4.3, we further post-process the generated samples to get individual test cases that are\\nsyntactically correct.', mimetype='text/plain', start_char_idx=0, end_char_idx=3351, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5604883565453653), NodeWithScore(node=TextNode(id_='9662bad8-69f3-449a-90b2-8504b4b939a4', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6514327296887347), NodeWithScore(node=TextNode(id_='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9662bad8-69f3-449a-90b2-8504b4b939a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6501219449053354), NodeWithScore(node=TextNode(id_='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9662bad8-69f3-449a-90b2-8504b4b939a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='80b71cf88cc7ad57bc804246e8c1e7e87c11388fff8a41c9068914570d456b4c')}, text='We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002\\nmay generate many trivial code solutions for the problems in APPS and CodeContests due to the\\nsuperior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to\\ndemonstrate the robustness of C ODET to this issue. Inspired by Chen et al. (2021) and Li et al.\\n(2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.\\n4.3 A NALYSIS ON TESTCASES\\nThe test cases are vital to C ODET since the core idea is based on test-driven execution agreement.\\nHence, in this subsection, we analyze the test cases by answering the following research questions.\\n6', mimetype='text/plain', start_char_idx=2687, end_char_idx=4153, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6909084127555096), NodeWithScore(node=TextNode(id_='9662bad8-69f3-449a-90b2-8504b4b939a4', embedding=None, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8c8bef48-889c-4bfc-afcb-20f7b3cb1336', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'codet_avaliacao.pdf', 'file_path': 'data\\\\codet_avaliacao.pdf', 'file_type': 'application/pdf', 'file_size': 1272933, 'creation_date': '2024-10-29', 'last_modified_date': '2024-10-29'}, hash='49e49b16d5043c68c1e5d71705484f7e0910ce7343a3aa45e1106756340520ca'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d1eeefea-9d44-4c9a-9758-52c2d23a44f0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55ee66c5fd6506114445289dc315c2f9e601fcdeb72ffcc873d1f035da4fe8e7')}, text='Published as a conference paper at ICLR 2023\\nMethods Baseline C ODET\\nk 1 10 50 100 1000 1 2 10 100\\nAPPSINTRODUCTORY 27.246.6 59 .4- - 34.67.441.253.26.6-\\nINTERVIEW 5.1 12.8 23 .0- - 8.13.0 11.218.15.3-\\nCOMPETITION 1.8 4.9 12 .1- - 2.20.4 4.1 8.63.7 -\\nCodeContests 0.7 3.0 5 .7 7.5 13 .92.11.4 2.3 5.32.3 9.92.4\\nTable 3: Pass@ k(%) results on the APPS and CodeContests benchmarks using code-davinci-002\\nin the zero-shot setting. The numbers in red indicate the absolute improvements of C ODET over\\nbaseline on pass@ 1, pass@ 10and pass@ 100. For C ODET, temperature is set to 0.8and sampling\\nnumber is set to 50for APPS and 1,000for CodeContests.\\ncode-davinci-002, the improvement is 18.8%, boosting the pass@ 1to65.8%, which is a 20+%ab-\\nsolute improvement over the best previously reported results (Inala et al., 2022). We attribute this\\nlarger improvement to the higher quality of test cases generated by code-davinci-002, providing a\\ndeeper analysis in Section 4.3. C ODET also achieves exceptional performance on the MBPP bench-\\nmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using\\nthe code-davinci-002 as an example, the pass@ 1improves by 9.6%. We also report pass@ 2and\\npass@ 10of C ODET to further show its superiority. The pass@ 2results of C ODET are close to\\nthe baseline pass@ 10results. Meanwhile, the improvements on pass@ 10are also consistently over\\n10% on the HumanEval benchmark.\\nThe experimental results of I NCODER -6B and C ODEGEN-MONO-16B further verify the effective-\\nness of C ODET. It is obvious C ODET can significantly improve the pass@ 1, with absolute improve-\\nments in the range of 4.2%to13.1%. INCODER -6B achieves the greatest improvement with a\\ngain of 13.1%on the MBPP benchmark. Similar to the experimental results of Codex, the pass@ 2\\nresults are close to the baseline pass@ 10. All the results demonstrate that C ODET can boost the\\nperformance of various pre-trained language models consistently.\\nAs for AlphaCode-C, it is consistently inferior to C ODET on both benchmarks using different mod-\\nels, demonstrating the superiority of our dual execution agreement that takes test case information\\ninto consideration. In addition, we notice that duplication exists in the generated code solutions and\\ntest cases. We perform an ablation study in Appendix D to show that de-duplication has little influ-\\nence on the results of C ODET. Moreover, we discuss the sensitivity of C ODET to the temperature in\\nAppendix E, showing the rationality of choosing a rather high temperature at 0.8.\\n4.2 R ESULTS ON APPS AND CODECONTESTS\\nWe also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We\\nbuild the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval\\nand MBPP by removing the example input-output cases in the problem descriptions. We employ\\ncode-davinci-002 for code solution and test case generation. The sampling number is set to 50for\\nAPPS to save computation cost on the 5,000testing problems, while for CodeContests, following\\nLi et al. (2022b), the sampling number is set to 1,000to solve especially hard problems. From the\\nresults summarized in Table 3, we can clearly observe the consistent performance improvements\\non both benchmarks using C ODET. The absolute pass@ 1improvement is 7.4%for introductory\\nproblems in APPS, while the improvements are not significant for competition level problems in\\nAPPS and CodeContest, indicating their difficulties.', mimetype='text/plain', start_char_idx=0, end_char_idx=3498, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6865730581017966)], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"\"\"\n",
    "    Answer the following questions about CodeT:\n",
    "    - What is the problem CodeT is solving?\n",
    "    - Why is solving this problem important?\n",
    "    - What is CodeT's contribution?\n",
    "    - Explain CodeT method\n",
    "    - How does CodeT perform compared to other frameworks?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 9\n",
      "file_name: codet_avaliacao.pdf\n",
      "file_path: data\\codet_avaliacao.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1272933\n",
      "creation_date: 2024-10-29\n",
      "last_modified_date: 2024-10-29\n",
      "\n",
      "Published as a conference paper at ICLR 2023\n",
      "of import statements, while the remaining problems are attributed to the failure of the model to\n",
      "understand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The\n",
      "correct understanding of the description “sum(first index value, last index value)” is to add the first\n",
      "and last values, while the code solutions that sum all values from the first to the last are ranked top\n",
      "1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration\n",
      "for future studies on improving code generation for more difficult programming problems.\n",
      "5 R ELATED WORK\n",
      "Code Generation with Large Models Recently, a number of large pre-trained language mod-\n",
      "els have been proposed for code generation. Benefiting from billions of trainable parameters and\n",
      "massive publicly available source code, models could achieve surprisingly good performance. For\n",
      "instance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-\n",
      "tors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot\n",
      "to provide real-time coding suggestions. Other open-source code generation models include GPT-\n",
      "Neo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022),\n",
      "PolyCoder (Xu et al., 2022), C ODEGEN(Nijkamp et al., 2022), and I NCODER (Fried et al., 2022).\n",
      "In our study, we take advantage of the Codex inference API provided by OpenAI as well as the two\n",
      "competitive open-source models C ODEGENand I NCODER to perform zero-shot code generation.\n",
      "Automatic Test Case Generation Automated test case generation for programming problems\n",
      "can reduce the effort of writing test cases manually by developers. Early works including Ran-\n",
      "doop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),\n",
      "DynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically\n",
      "generate test cases for statically typed programming languages like Java. The later proposed Pyn-\n",
      "guin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-\n",
      "theless, they are all search-based heuristics methods, which have limitations to the diversity and\n",
      "quantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano\n",
      "et al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)\n",
      "and T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous\n",
      "works that require heuristic rules or model training, we directly sample test cases from powerful\n",
      "code generation models like Codex in the zero-shot setting with elaborate prompts.\n",
      "Code Selection from Multiple Samples Despite large models have achieved great performance in\n",
      "code generation, the models need to sample many times to find the correct answer. Recently, several\n",
      "approaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe\n",
      "et al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed\n",
      "to jointly train the generator and ranker through a multi-task framework. In the domain of general\n",
      "purpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has\n",
      "been proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022;\n",
      "Lahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or\n",
      "user interactions, we let the large models generate test cases for themselves and automatically rank\n",
      "the solutions based on the test-driven dual execution agreement. The idea of ranking based on\n",
      "agreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).\n",
      "6 C ONCLUSION AND FUTURE WORK\n",
      "In this paper, we propose a simple yet effective approach, called C ODET, leveraging pre-trained\n",
      "language models to generate both the code solutions and the test cases. C ODET executes the code\n",
      "solutions using the test cases and chooses the best solution based on the dual execution agreement.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
